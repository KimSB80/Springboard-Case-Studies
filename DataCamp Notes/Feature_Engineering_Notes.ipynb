{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f81a79-0f35-4f53-826d-febeb1e6cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ea009-9e2e-42fe-8d7f-59148aa7147b",
   "metadata": {},
   "source": [
    "## 1. Imputation\n",
    "Dealing with missing values. There is not an optimum threshold for dropping but you can use 70% as an example value and try to drop the rows and columns which have missing values with higher than this threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5e84d-ccae-46cc-8f2c-c03c2288badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]\n",
    "\n",
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())\n",
    "\n",
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts()\n",
    ".idxmax(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43d955-89c9-4021-8d10-b326ce19fa05",
   "metadata": {},
   "source": [
    "## 2. Handling Outliers\n",
    "Outlier Detection with Standard Deviation\n",
    "If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?\n",
    "\n",
    "There is no trivial solution for x, but usually, a value between 2 and 4 seems practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ef7fb-7bdd-446e-b0cf-9ec1bc269101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d2f8f-a219-40d1-bd9a-04f369ca2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a31f9-2471-47d8-9b69-d19b7c5def09",
   "metadata": {},
   "source": [
    "## 3. Binning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350247fa-1cbc-4b17-932d-81bfa512c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Binning Example\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "\n",
    "#Categorical Binning Example\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcf171-4123-4623-9dac-f960654e541a",
   "metadata": {},
   "source": [
    "## 4. Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613338f4-2c52-485a-b464-c6f189b6f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39653c-a322-43fc-a123-f857eac81ec2",
   "metadata": {},
   "source": [
    "## 5. One-Hot Encoding (Categorical Column Grouping)\n",
    "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3793350-470e-43d0-9cad-63c434c53d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b511c-bef7-491d-a714-7562ec7c6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "data = data.join(encoded_columns).drop('column', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405242ab-5203-4467-bc8a-5e0c6e892b91",
   "metadata": {},
   "source": [
    "## 6. Grouping Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca8b68-f811-4968-87e8-3666f12b623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical column grouping\n",
    "#select the label with the highest frequency\n",
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde9b03-1062-4174-a8ad-35771945d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275c76e-e9ce-4af6-83e8-46c8f43340c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical data grouping\n",
    "\n",
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f87caa-48fc-47af-b106-8864be4cd2ae",
   "metadata": {},
   "source": [
    "## 7. Feature Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20fc24-405f-458e-814d-27693e478c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.name\n",
    "0  Luther N. Gonzalez\n",
    "1    Charles M. Young\n",
    "2        Terry Lawson\n",
    "3       Kristen White\n",
    "4      Thomas Logsdon\n",
    "#Extracting first names\n",
    "data.name.str.split(\" \").map(lambda x: x[0])\n",
    "0     Luther\n",
    "1    Charles\n",
    "2      Terry\n",
    "3    Kristen\n",
    "4     Thomas\n",
    "#Extracting last names\n",
    "data.name.str.split(\" \").map(lambda x: x[-1])\n",
    "0    Gonzalez\n",
    "1       Young\n",
    "2      Lawson\n",
    "3       White\n",
    "4     Logsdon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723d1ca-7cb5-48ec-b186-193c7d3515ad",
   "metadata": {},
   "source": [
    "## 8. Scaling\n",
    "Data columns should have similar ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8738ce-b8e8-4bf0-aefd-9b3dd3331b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. \n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe437ed-97da-46ca-9b4e-79ce7d0cada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (or z-score normalization) scales the values while taking into account standard deviation. \n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231af79e-76f1-4b09-9069-ee413d633fae",
   "metadata": {},
   "source": [
    "## 9. Extracting Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0604347-27a1-4da3-acf7-d4b6c260ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
